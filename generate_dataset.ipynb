{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.integration.openai import autolog\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"amae\", \"job_type\": \"generation\", \"entity\": \"chamera\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a retry behavior in case we hit the API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_NAME = \"gpt-4\" # \"gpt-4-0613\" as of 2024-01-16\n",
    "MODEL_NAME = \"gpt-4-1106-preview\" # the updated version from DevDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You're an AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is your knowledge cutoff?\"},\n",
    "        ]\n",
    "print(\"Calling OpenAI API...\")\n",
    "example_response = completion_with_backoff(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    n = 1,\n",
    "    )\n",
    "print(\"OpenAI API call completed\")\n",
    "example_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read system_template.txt file into an f-string\n",
    "with open(\"system_template.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompt_template.txt file into an f-string\n",
    "with open(\"prompt_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(data_dir):\n",
    "    pdf_files = list(map(str, Path(data_dir).glob(\"*.pdf\")))\n",
    "    documents = []\n",
    "    for file_path in pdf_files:\n",
    "        try:\n",
    "            documents.extend(PyPDFLoader(file_path=file_path, extract_images=True).load())\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path} due to {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load docs\\2911529a-b08d-4367-8e0d-f8c3d2514e81-471547d9-e1b5-4383-a7e8-ea237642db5c.pdf due to cannot reshape array of size 1298 into shape (118,81,newaxis)\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "tokens_per_document = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
    "\n",
    "# generate a histogram of tokens per document\n",
    "plt.hist(tokens_per_document, bins=50)\n",
    "plt.savefig(\"generate_ds_tokens_per_document.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a random chunk from a document\n",
    "def extract_random_chunk(document, max_tokens=1024):\n",
    "    if isinstance(document, Document):\n",
    "        tokens = tokenizer.encode(document.page_content)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return document.page_content\n",
    "    else:\n",
    "        tokens = tokenizer.encode(document)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return document\n",
    "    start = random.randint(0, len(tokens) - max_tokens)\n",
    "    end = start + max_tokens\n",
    "    return tokenizer.decode(tokens[start:end])\n",
    "\n",
    "chunk = extract_random_chunk(documents[300])\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"\\nNew question:\\n\" # tab separated queries\n",
    "with open(\"question_examples.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    real_queries = data.split(delimiter)\n",
    "\n",
    "print(real_queries)\n",
    "print(len(real_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "How should I properly load the washing machine to prevent imbalance? How do I adjust or remove the shelves for     \n",
       "cleaning or storage optimization in regards to my refrigerator? Why is my 2005 Honda Accord's steering wheel       \n",
       "shaking?                                                                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of a home appliance or Honda Accord 2005 car manual documentation. This \n",
       "will serve as inspiration for synthetic user question and the source of the answer. Here is the document fragment: \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "Your vehicle is equipped with many features that work together toprotect you and your passengersduring a crash.    \n",
       "Some features do not require any action on your part. These include astrong steel framework that forms asafety cage\n",
       "around the passengercompartment; front and rear crushzones; a collapsible steering column;and tensioners that      \n",
       "tighten the frontseat belts in a crash. However, you and your passengers can’t take full advantage of thesefeatures\n",
       "unless you remain sitting ina proper position and . In fact, some safety features can contribute to injuries ifthey\n",
       "are not used properly. The following pages explain how you c a nt a k ea na c t i v er o l ei np r o t e c t i n   \n",
       "gyourself and your passengers.Your Vehicle’s Safety Features always wear your seat beltsDriver and Passenger Safety\n",
       "7(1) (2) (2)(3) (4) (5)(7)(8) (7)(10) (11)(9) (6)(9)(6) (1) Safety Cage (2) Crush Zones(3) Seats and Seat-Backs(4) \n",
       "Head Restraints(5) Collapsible Steering Column (6) Seat Belts(7) Front Airbags(8) Side Airbags(9) Side Curtain     \n",
       "Airbags(10) Door Locks(11) Front Seat Belt Tensionerš̐ʗ̌̔ʗ̌̑ɹ̍̌ɿ̏̑ɿ̌̎ɹ̨̙̥̏̍̒̎̌ɹ̌̌̍̌ɹ                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer to the user question using the documentation. You'll be evaluated on:                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how realistic is that this question will come from a real user one day?                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>is this question about a car or a home appliance?                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>can the question be answered using the document fragment above?                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>impersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "How should I properly load the washing machine to prevent imbalance? How do I adjust or remove the shelves for     \n",
       "cleaning or storage optimization in regards to my refrigerator? Why is my 2005 Honda Accord's steering wheel       \n",
       "shaking?                                                                                                           \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of a home appliance or Honda Accord 2005 car manual documentation. This \n",
       "will serve as inspiration for synthetic user question and the source of the answer. Here is the document fragment: \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "Your vehicle is equipped with many features that work together toprotect you and your passengersduring a crash.    \n",
       "Some features do not require any action on your part. These include astrong steel framework that forms asafety cage\n",
       "around the passengercompartment; front and rear crushzones; a collapsible steering column;and tensioners that      \n",
       "tighten the frontseat belts in a crash. However, you and your passengers can’t take full advantage of thesefeatures\n",
       "unless you remain sitting ina proper position and . In fact, some safety features can contribute to injuries ifthey\n",
       "are not used properly. The following pages explain how you c a nt a k ea na c t i v er o l ei np r o t e c t i n   \n",
       "gyourself and your passengers.Your Vehicle’s Safety Features always wear your seat beltsDriver and Passenger Safety\n",
       "7(1) (2) (2)(3) (4) (5)(7)(8) (7)(10) (11)(9) (6)(9)(6) (1) Safety Cage (2) Crush Zones(3) Seats and Seat-Backs(4) \n",
       "Head Restraints(5) Collapsible Steering Column (6) Seat Belts(7) Front Airbags(8) Side Airbags(9) Side Curtain     \n",
       "Airbags(10) Door Locks(11) Front Seat Belt Tensionerš̐ʗ̌̔ʗ̌̑ɹ̍̌ɿ̏̑ɿ̌̎ɹ̨̙̥̏̍̒̎̌ɹ̌̌̍̌ɹ                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer to the user question using the documentation. You'll be evaluated on:                  \n",
       "\n",
       "\u001b[1;33m • \u001b[0mhow realistic is that this question will come from a real user one day?                                         \n",
       "\u001b[1;33m • \u001b[0mis this question about a car or a home appliance?                                                               \n",
       "\u001b[1;33m • \u001b[0mcan the question be answered using the document fragment above?                                                 \n",
       "\u001b[1;33m • \u001b[0mhow accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "\u001b[1;33m   \u001b[0mimpersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_context_prompt(chunk, n_questions=3):\n",
    "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
    "    user_prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
    "    return user_prompt\n",
    "\n",
    "user_prompt = generate_context_prompt(chunk)\n",
    "Markdown(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document, generate n (n_generations parameter) questions and corresponding answers\n",
    "# the answers will be treated as ideal answers for the generated questions \n",
    "def generate_questions_and_answers(documents, n_questions=3, n_generations=5):\n",
    "    questions = []\n",
    "    for document in documents:\n",
    "        chunk = extract_random_chunk(document)\n",
    "        user_prompt = generate_context_prompt(chunk, n_questions)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        print(\"Calling OpenAI API...\")\n",
    "        response = completion_with_backoff(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            n = n_generations,\n",
    "            )\n",
    "        print(\"OpenAI API call completed\")\n",
    "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
    "def parse_generation(generation):\n",
    "    lines = generation.split(\"\\n\")\n",
    "    context = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    flag = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"CONTEXT:\" in line:\n",
    "            flag = \"context\"\n",
    "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
    "        elif \"QUESTION:\" in line:\n",
    "            flag = \"question\"\n",
    "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif \"ANSWER:\" in line:\n",
    "            flag = \"answer\"\n",
    "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
    "\n",
    "        if flag == \"context\":\n",
    "            context.append(line)\n",
    "        elif flag == \"question\":\n",
    "            question.append(line)\n",
    "        elif flag == \"answer\":\n",
    "            answer.append(line)\n",
    "\n",
    "    context = \"\\n\".join(context)\n",
    "    question = \"\\n\".join(question)\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling OpenAI API...\n",
      "OpenAI API call completed\n",
      "New generation:\n",
      "(\"The user is on a road trip, and plans to charge his phone and a camping light through his car's accessory power sockets. However, he also wants to use his car lighter. He's unsure of the power limitations, and whether all devices can operate simultaneously.\\n\", 'Can I use my phone charger in the car socket while using the light? Also, can I use the lighter at the same time?\\n', 'Yes, you can use your phone charger and camping light at the same time, as long as their combined power rating does not exceed 120 watts, or 10 amps. However, the sockets in your car cannot power an automotive type cigarette lighter. Always remember to switch the ignition on to either ACCESSORY (I) or ON (II) to use the power sockets.\\n')\n",
      "\n",
      "\n",
      "New generation:\n",
      "(\"The user recently purchased a used 2005 Honda Accord. He notices that the vehicle has accessory power sockets, but he is unsure about their use and the amount of power they could supply. He is particularly interested to know if it's possible to charge his mobile phone using these sockets.\\n\", 'Can I use the accessory power sockets in my 2005 Accord to charge my mobile phone? And, do I need to have the ignition on for them to work?\\n', \"Yes, you can certainly use the accessory power sockets in your car to charge your mobile phone, as these sockets supply power for 12 volt DC accessories that are rated 120 watts or less. Moreover, your mobile phone's charger is usually well below this limit. To use these power sockets, you need to switch the ignition to either ACCESSORY (I) or ON (II) position. Make sure not to overuse the sockets, if more than one socket is being used, the combined power rating of the connected accessories should be 120 watts or less.\")\n",
      "\n",
      "\n",
      "New generation:\n",
      "('\\nThe user has a long road trip ahead and they bought a new 12V in-car fridge for the journey. They are concerned about how much power it draws and whether they could also use another device at the same time in the car, like a 12V phone charger. \\n', '\\nCan I use my 12V fridge and my phone charger at the same time in my 2005 Accord? They both go into the accessory power sockets. How much power can these sockets handle?\\n', \"\\nYour Honda Accord's accessory power sockets are capable of supplying power to 12-volt DC accessories that are rated 120 watts or less, or 10 amps. Therefore, you can use your 12V fridge and your phone charger at the same time, as long as their combined power rating doesn't exceed 120 watts or 10 amps. Also, bear in mind that the ignition switch must be in ACCESSORY (I) or ON (II) to use any of the accessory power sockets.\")\n",
      "\n",
      "\n",
      "New generation:\n",
      "('\\nThe user is driving his Honda Accord 2005 and wants to place his sunglasses somewhere. He has seen the sunglasses holder but he is unsure about how to use it. \\n', \"\\nHi, I'm driving a 2005 Honda Accord. Where can I put my sunglasses, and how do I use the holder?\\n\", \"\\nIn your 2005 Honda Accord, there is a sunglasses holder for you to keep your sunglasses. To open the sunglasses holder, simply push on the front edge. Please note that it's important the holder is closed while you are driving. Some larger styles of sunglasses may not fit in the holder. You may also store small items in this holder, just make sure they are small enough to let the holder close and latch, and that they are not heavy enough to cause the holder to pop open while driving.\")\n",
      "\n",
      "\n",
      "New generation:\n",
      "(\"\\nThe user recently purchased a second-hand Honda Accord 2005. During the nighttime, he needs to charge his phone. However, he is unsure of the power capabilities of the car's in-built power sockets or how to use them. \\n\", '\\nHey, can I charge my phone using the power sockets in my 2005 Honda Accord? And how do I turn them on?\\n', '\\nSure, your 2005 Honda Accord has power sockets that supply energy for 12 volt DC accessories that are rated 120 watts or less. To use these sockets, your ignition switch must be in the ACCESSORY (I) or ON (II) position. However, ensure that when more than one socket is being used, the combined power rating of the accessories should be 120 watts or less.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generations = generate_questions_and_answers([documents[100]], n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    print(\"New generation:\")\n",
    "    print(parse_generation(generation))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_documents(documents):\n",
    "    # filter out documents that are likely to be tables or pages with images\n",
    "    non_relevant_documents = [document for document in documents if document.page_content.count('.') < 200]\n",
    "    # get the top 3% of documents by tokens count\n",
    "    top_documents = sorted(non_relevant_documents, key=lambda x: len(tokenizer.encode(x.page_content)),\n",
    "                           reverse=True)[:int(0.03 * len(non_relevant_documents))]\n",
    "    return top_documents\n",
    "\n",
    "len(get_best_documents(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_generations = []\n",
    "generations = generate_questions_and_answers(get_best_documents(documents), n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    context, question, answer = parse_generation(generation)\n",
    "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "# let's convert parsed_generations to a pandas dataframe and save it locally\n",
    "df = pd.DataFrame(parsed_generations)\n",
    "df.to_csv('generated_examples.csv', index=False)\n",
    "\n",
    "# log df as a table to W&B for interactive exploration\n",
    "wandb.log({\"generated_examples\": wandb.Table(dataframe=df)})\n",
    "\n",
    "wandb.log({\"openai_gen_model\": example_response.model})\n",
    "\n",
    "# log csv file as an artifact to W&B for later use\n",
    "artifact = wandb.Artifact(\"generated_examples\", type=\"dataset\")\n",
    "artifact.add_file(\"generated_examples.csv\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>usage/completion_tokens</td><td>▁▃▄▄▃▄▃▂▇▆▂▄▃▄█▄▃▁▄▂▄▃▃▆▃▁▂▅▄▅▄▄</td></tr><tr><td>usage/elapsed_time</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>usage/prompt_tokens</td><td>▄▆▄▅▃▇▄▄▄▁▄▄▃▄▄▅▄▁▅▄▄▇▂▅█▅▃█▃▇▆▄</td></tr><tr><td>usage/total_tokens</td><td>▁▃▄▄▃▄▃▂▇▅▂▄▃▄█▄▃▁▄▂▄▃▃▆▃▁▂▅▄▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>openai_gen_model</td><td>gpt-4-1106-preview</td></tr><tr><td>usage/completion_tokens</td><td>1184</td></tr><tr><td>usage/elapsed_time</td><td>0.0</td></tr><tr><td>usage/prompt_tokens</td><td>1398</td></tr><tr><td>usage/total_tokens</td><td>2582</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-cloud-30</strong> at: <a href='https://wandb.ai/chamera/amae/runs/zdojk9bq' target=\"_blank\">https://wandb.ai/chamera/amae/runs/zdojk9bq</a><br/>Synced 5 W&B file(s), 33 media file(s), 34 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240116_135741-zdojk9bq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round1:\n",
    "\n",
    "Input:\n",
    "- pages of Honda Accord manual\n",
    "- each page had to be a non-content table page\n",
    "- top 5% pages with the most tokens available\n",
    "- resulted in 14 pages\n",
    "- 3 questions per page\n",
    "- 5 generations per page\n",
    "- gpt-4-0613 model\n",
    "\n",
    "Output:\n",
    "- API calls cost: 1.34$\n",
    "\n",
    "### Round 2:\n",
    "\n",
    "Input:\n",
    "- pages of Honda Accord, washing machine, dishwasher, oven, refrigerator, induction hob manuals\n",
    "- each page had to be a non-content table page\n",
    "- top 3% pages with the most tokens available\n",
    "- resulted in 32 pages\n",
    "- 3 questions per page\n",
    "- 5 generations per page\n",
    "- gpt-4-1106-preview model\n",
    "\n",
    "Output:\n",
    "- API calls cost: 0.95$\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
